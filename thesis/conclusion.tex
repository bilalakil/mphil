\chapter{Conclusion}

  We observed that Apache Hadoop MapReduce \cite{HADOOP:HOMEPAGE} remained a dominant choice for distributed computing in the bioinformatics field -- and likely in other scientific disciplines -- in comparison to the newer, more dataflow oriented Apache Spark \cite{SPARK:HOMEPAGE,ZahariaCFSS:HotCloud10} and Apache Flink \cite{FLINK:HOMEPAGE,CarboneKEMHT:DEBU2015}. Our impression was that in these disciplines where practitioners are not generally equipped with deep distribution computing knowledge, it is difficult to keep up with the rapid pace of innovation in dataflow engines, especially where there exists a lack of academic guidance in matters other than performance. While performance is indeed always a concern, we argue that it is less so within fields where users of these distributed systems are not distributed computing experts themselves. Other concerns including usability, which we define as the ease of learning to use and becoming proficient with a system, and practicality, which we define as the flexibility of a system in terms of adopting or fitting it into one's existing development environment and workflow, become very important for users who do not have the ability to learn complex distributed computing topics on a whim, or configure clusters or computing environments as they please.
  
  Thus our research firstly investigated this hypothesis -- that Hadoop MapReduce was indeed a dominant choice in bioinformatics and other scientific disciplines, over newer, more performant and dataflow oriented systems. With that validated, we proceeded to both confirm that these newer systems are indeed more suitable for problem solving in these sub-disciplines, and then to explore potential solutions that could increase adoption of these newer systems in consideration of that. Effectively, we have bridged the gap which we identified as missing non-performance related comparisons between these engines, in the hope that this information will provide guidance to practitioners in said disciplines, granting them the confidence to work with the systems most suitable for them, rather than resorting to MapReduce by default -- ultimately helping the disciplines to reap the benefits of the unending innovations of the distributed computing field.
  
  In summary, to achieve these goals we:
  
  \begin{itemize}
    \item Performed a thorough literature review, and created a web scraper to analyse a large number of bioinformatics related research -- thus validating our hypothesis that this problem in fact existed.
    \item Performed a novel large-scale usability study to confirm that Apache Spark and Apache Flink were indeed more suitable for distributed computing tasks within scientific disciplines like bioinformatics. This doubled up as a large part of the multidimensional software comparison that was developed.
    \item Proposed a methodology for developing multidimensional software comparisons (considering the lack of an already well-developed one). The output comparisons are intended to facilitate effective communication of a wide range of considerations, and provide the individual reader the ability to conveniently navigate and even appropriate the comparison results based on their specific use case.
    \item Applied this proposed methodology, along with usability data from the large-scale usability study, performance data from \citeauthor{VEIGA:EVALUATION:2015} \cite{VEIGA:EVALUATION:2015}, and researched practicality data, to produce a multidimensional comparison of Hadoop MapReduce, Spark and Flink -- thus bridging the identified gap in knowledge.
  \end{itemize} \medskip
  

  In more detail, we implemented a web scraper that was used to scrape one year of data from the BMC Bioinformatics journal, which we combined with an analysis of papers from three years of more sparse bioinformatics research presented within the IEEE BigData conference. With this information we examined the systems being used in recent bioinformatics research, considering both purely bioinformatics perspectives (within the BMC Bioinformatics journal), as well as from a more distributed computing perspective but with bioinformatics applications (within the IEEE BigData conference). This practice was effective, successfully providing a holistic view of the technical landscape in the bioinformatics field (at least pertaining to distributed computing), and accordingly validating our hypothesis that newer dataflow engines were struggling to gain traction within the field. With more time, it could be valuable to negotiate with other representative journals which may not be open access like BMC Bioinformatics, to facilitate scraping of a wider range of data -- however even without doing so, we consider our findings quite reliable. \medskip
  

  We performed a large-scale usability study with students in a cloud computing class, which addressed the lack of usability data concerning modern distributed data processing platforms, and act as the dominant part of the usability consideration in our multidimensional comparison. The usability study primarily involved survey data collected from three surveys -- one following completion of each of three data analysis assignments. The first assignment used MapReduce, and the latter two employed a crossed A/B test with Spark and Flink. To the best of our knowledge, this usability study was the first of its kind, and so substantial effort was placed into ensuring its reliability and making it a quality contribution to this research.
  
  The usability study worked well: study participation was high; students cooperated in the crossed A/B test with perhaps surprisingly little organisational difficulty; only a small portion of survey data was left unfilled; and student course satisfaction levels remained high (data not shown). Catering for the diversity of a class with both IT and data science students is a challenge, and we see the learned lessons and careful design of our study as one of our contributions.
  
  We found that participants' perceived preferences were strongly in favour of either Spark or Flink in comparison to MapReduce, however there was little difference between the two modern systems themselves. There was also no significant difference in the amount of time participants reported they required to complete the assignments using either of the modern systems. Thus from a usability point of view and without a more specific use case in mind, both Spark and Flink seem to be equally suitable choices over MapReduce, most likely due to the high-level nature of these data processing platforms.
  
  In performing our usability study, we experimented with using the System Usability Scale (SUS) \cite{BROOKE:SUS:1996} to measure and compare the usability of the three systems, which we have not seen used in programming contexts despite being found effective elsewhere \cite{BANGOR:SUS_EVALUATION:2008}. However despite its convenience, we ultimately did not find it highly effective, as it did not provide much insight into the usability of each system, nor did it correlate with perceived preferences. Looking in detail at responses to individual SUS questions highlighted weaknesses in the learn-ability and need for support in all three systems, supporting what we believe to be key areas for improvement in data science systems which would greatly support their adoption by users with non-computing backgrounds: first-party documentation and the process of debugging programs.
  
  A further finding when performing the usability study was that participants preferred the first of the data processing engines that they encountered in the class -- either Spark or Flink. Interestingly, there was also a significant difference in SUS scores between the assignments, however there was no suitable data to highlight the cause of this difference. We suspect that it is a combination of a first-used advantage and differences in assignment difficulty.
  
  However, Python with Flink was not working to an acceptable degree and we thus could not provide support and learning resources for them. We recognise that this would have introduced some bias to the study, and to account for this analyses were repeated with Python user data excluded, where it became apparent that among Java users Flink was more often reported as preferred than Spark. A subsection of the usability study's analysis was dedicated to examining these potentially introduced biases, where we found that all of our initial conclusions remained consistent in the Java-only pool, indicating that little bias was introduced.
  
  Overall, we consider the usability study a success in that we could measure meaningful differences between big data processing systems in their usability, which might contribute to their adoption as much as technical aspects or raw performance -- especially considering that all of them scale well when adding more virtual machines. There were indeed some challenges faced during execution of the study, leading us to advise allowing a generous amount of time for preparation, and if at all possible to avoid having to continue preparation after the usability study's commencement -- but that was the nature of our situation and we believe we executed it well, all things considered. With experience, we now hold the opinion that the medium of university classes is an excellent one for performing software comparisons, and recommend other researchers to consider doing the same if that is indeed an option. \medskip
  
  
  We then continued to produce a clear, reliable comparison of these three systems, helping to highlight the strengths and weaknesses of each, and guiding practitioners in making informed decisions regarding the system that would best fit a given project. The comparison did not solely consider the context of bioinformatics, but of data science and distributed data analysis in general, because this phenomenon is likely not limited to the single scientific field.

  The comparison was multidimensional because performance is not the only factor necessary to consider when deciding which system to use -- as is especially the case for less technical audiences, such as data scientists, who may struggle to adopt cutting edge systems substantially more than, say, a computer scientist or distributed computing researcher. The need grows further considering the compared software, wherein by their nature, all distributed systems will be able to scale and achieve immense speeds by adding more virtual machines; so it would take more than only performance to differentiate or decide between these systems.
  
  On a larger scale, the presence of regular, high quality multidimensional comparisons would improve the pace of innovation in scientific fields, which are not naturally focused on keeping up with the cutting edge of the software they may be using. Thus this particular comparison is useful now, considering the lack of related research, but needs to be kept relevant with more comparisons that follow the ongoing developments of the distributed computing field.

  To facilitate both this research, and to promote and assist in the conduct of ongoing comparisons of software in general, we proposed a methodology for performing multidimensional software comparisons -- which we were unable to find an existing standard for. This research also features its first application: the comparison of MapReduce, Spark and Flink, in consideration of performance, usability and practicality for data science.

  The methodology was highly effective in this application, where it provided both guidance and structure in devising, performing, and displaying the multidimensional comparison results. Already a valuable contribution, we are excited to follow the growth of the methodology as it is applied in varying contexts. We found that it was indeed general enough to be applied to a variety of situations, while still providing useful guidance in how we could massage our collected data into a format navigable by readers with a variety of needs. We found that our choice of performance, usability and practicality as high-level considerations was effective, as all relevant metrics seemed to naturally fall into one of those three. If repeated, we would consider doing more case studies relating to the practicality consideration, perhaps by visiting actual researchers within relevant sub-disciplines, and learning what the prohibiting factors are in trying to adopt a new system in reality as opposed to anecdotally.

  The performed multidimensional comparison itself combined existing research from \citeauthor{VEIGA:EVALUATION:2015} \cite{VEIGA:EVALUATION:2015} for the performance consideration, with both our large-scale usability study and more simple static analyses for the usability and practicality considerations.
  
  The results align with our hypothesis that Spark and Flink are indeed superior to MapReduce both in terms of performance and usability -- in fact being significantly so. While MapReduce exhibited slightly improved scalability to Spark, its speed of execution was far behind the other systems. Owing to MapReduce's support of arbitrary programming languages, it scored similar to Spark and Flink in consideration of practicality.

  Spark and Flink turned out similar across all three considerations. With that being said, one of the proposed methodology's goals is to provide the output comparison's readers with the flexibility to adjust the weighting of involved measurements. We especially recommend doing so here, in the hopes of providing differentiation between the two systems to better suit the reader's individual circumstances. Notably, Spark exhibited superior performance in the form of speed, but inferior in the form of scalability, compared to Flink. We believe this comparison achieves our goal of empowering readers to understand the differences between these systems, not only in consideration of performance, and thus hopefully provide confidence in making the switch more appropriate systems where applicable. \medskip
  
  
  Overall we consider this research a success, as none of its hypotheses were invalidated, and all of its goals were reasonably met. It forms the first step by our group, and an early step in the field as a whole, to better understand the usability and adoption of big data processing systems for data science. Our long-term goal is to identify factors and to develop techniques for improving the usability and impact of the next generation of big data systems. The proposed methodology will hopefully grow and serve an even wider goal, of promoting ongoing execution of software comparisons. This is necessary to assist the many scientific disciplines, such as bioinformatics, in using the most appropriate software for their research, and thus increasing their pace of innovation.